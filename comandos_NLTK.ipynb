{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alumno: Jesus Eduardo Ceballos Contrereas\n",
    "# Comandos NLTK\n",
    "# 22/04/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de texto:\n",
    "- word_tokenize(texto): Divide un texto en tokens (palabras o unidades significativas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "texto = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentación de oraciones:\n",
    "- sent_tokenize(texto): Divide un texto en oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "texto = \"NLTK es una biblioteca de Python. Es útil para procesamiento de lenguaje natural.\"\n",
    "oraciones = sent_tokenize(texto)\n",
    "print(oraciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de palabras:\n",
    "- regexp_tokenize(texto, patrón): Divide un texto en tokens usando una expresión regular.\n",
    "- WhitespaceTokenizer().tokenize(texto): Divide un texto en tokens usando espacios en blanco como separadores.\n",
    "- TreebankWordTokenizer().tokenize(texto): Tokeniza un texto siguiendo las convenciones del Penn Treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "texto = \"NLTK es una biblioteca de Python.\"\n",
    "tokens = regexp_tokenize(texto, pattern='\\w+')\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming:\n",
    "- PorterStemmer().stem(palabra): Reduce una palabra a su raíz (forma truncada).\n",
    "- SnowballStemmer('idioma').stem(palabra): Similar al Porter Stemmer pero con soporte para varios idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "palabra = \"running\"\n",
    "stem = stemmer.stem(palabra)\n",
    "print(stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematización:\n",
    "- WordNetLemmatizer().lemmatize(palabra, pos='a'): Reduce una palabra a su forma base o lema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "print(lemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de stopwords:\n",
    "- stopwords.words('idioma'): Lista de stopwords para un idioma específico.\n",
    "- stopwords.words('idioma'): Elimina palabras comunes que no aportan mucho significado al análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokens = word_tokenize(texto)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos:\n",
    "- SentimentIntensityAnalyzer().polarity_scores(texto): Determina la polaridad (positiva/negativa) de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "text = \"NLTK es increíblemente útil para el análisis de texto.\"\n",
    "sentiment = analyzer.polarity_scores(text)\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging (Etiquetado gramatical):\n",
    "- pos_tag(tokens): Etiqueta las partes del discurso de las palabras en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking:\n",
    "- RegexpParser(patrones).parse(etiquetas_pos): Agrupa tokens en fragmentos gramaticales más grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "grammar = r'NP: {<DT>?<JJ>*<NN>}'  # Define la gramática para NP (frase nominal)\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "chunks = chunk_parser.parse(pos_tags)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER):\n",
    "- ne_chunk(etiquetas_pos): Identifica entidades nombradas en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "ner_chunks = ne_chunk(pos_tags)\n",
    "print(ner_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras más frecuentes:\n",
    "- FreqDist(tokens): Calcula la frecuencia de cada token en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "freq_dist = FreqDist(tokens)\n",
    "print(freq_dist.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de frecuencia de palabras:\n",
    "- FreqDist(tokens): Calcula la frecuencia de cada token en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "freq_dist = FreqDist(tokens)\n",
    "freq_dist.plot(30, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations:\n",
    "- BigramCollocationFinder.from_words(tokens): Encuentra colocaciones (pares de palabras frecuentes) en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "collocations = finder.nbest(lambda bigram, freq: freq, 5)\n",
    "print(collocations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet:\n",
    "- synsets(palabra): Obtiene los synsets (conjuntos de sinónimos) para una palabra.\n",
    "- synset('nombre.synset'): Obtiene información sobre un synset específico.\n",
    "- synset('nombre.synset').definition(): Obtiene la definición de un synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synsets = wordnet.synsets(\"car\")\n",
    "for synset in synsets:\n",
    "    print(synset.definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus:\n",
    "- PlaintextCorpusReader(directorio, 'patrón'): Crea un objeto de corpus a partir de archivos de texto plano en un directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/ruta/al/directorio'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
    "print(corpus.fileids())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de archivos PDF:\n",
    "- PlaintextCorpusReader(directorio, '.*\\.pdf'): Extracción de texto de archivos PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/ruta/al/directorio'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*\\.pdf')\n",
    "for file_id in corpus.fileids():\n",
    "    text = corpus.raw(file_id)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de archivos XML:\n",
    "- XMLCorpusReader(directorio, 'patrón'): Procesamiento de archivos XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import XMLCorpusReader\n",
    "corpus_root = '/ruta/al/directorio'\n",
    "corpus = XMLCorpusReader(corpus_root, '.*\\.xml')\n",
    "print(corpus.fileids())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de archivos CSV:\n",
    "- CSVCorpusReader(archivo): Lectura de archivos CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import CSVCorpusReader\n",
    "corpus_root = '/ruta/al/archivo.csv'\n",
    "corpus = CSVCorpusReader(corpus_root)\n",
    "for row in corpus:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de n-gramas:\n",
    "- ngrams(tokens, n): Genera n-gramas (secuencias de n palabras consecutivas) a partir de una lista de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "bigrams = ngrams(tokens, 2)\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution Plot:\n",
    "- FreqDist(tokens).plot(): Gráfico de la distribución de frecuencia de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"NLTK es una biblioteca de Python para el procesamiento de lenguaje natural.\"\n",
    "tokens = word_tokenize(text)\n",
    "freq_dist = FreqDist(tokens)\n",
    "freq_dist.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridad de palabras:\n",
    "- path_similarity(synset1, synset2): Calcula la similaridad de caminos entre dos synsets.\n",
    "- wup_similarity(synset1, synset2): Calcula la similaridad Wu-Palmer entre dos synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word1 = wordnet.synset('car.n.01')\n",
    "word2 = wordnet.synset('bike.n.01')\n",
    "similarity = word1.path_similarity(word2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Analysis:\n",
    "- morphy(palabra, pos='n'): Realiza análisis morfológico en una palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "print(lemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification:\n",
    "- NaiveBayesClassifier.train(datos_entrenamiento): Entrenamiento de un clasificador de texto usando el algoritmo Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def document_features(document):\n",
    "    features = {}\n",
    "    for word in word_tokenize(document):\n",
    "        features[word] = True\n",
    "    return features\n",
    "\n",
    "training_data = [(\"NLTK is a Python library for natural language processing.\", \"positive\"),\n",
    "                 (\"I found NLTK documentation very helpful.\", \"positive\"),\n",
    "                 (\"The NLTK book is a great resource.\", \"positive\"),\n",
    "                 (\"I am struggling with NLTK installation.\", \"negative\"),\n",
    "                 (\"I can't understand NLTK's API.\", \"negative\")]\n",
    "featuresets = [(document_features(text), label) for (text, label) in training_data]\n",
    "classifier = NaiveBayesClassifier.train(featuresets)\n",
    "text_to_classify = \"I am trying to learn NLTK.\"\n",
    "print(classifier.classify(document_features(text_to_classify)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
